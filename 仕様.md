# 実装クラス
* Agentクラス・・・Unity内のActorとデータをやり取りする。1episodeごとにデータをBufferに送る。
* Bufferクラス・・・Agentから受け取ったデータをBatchにする。
* Trainerクラス・・・BufferクラスからBatchを受け取りNetworkを更新する。
* BrainServerクラス・・・Brainからリクエストを受け取りONNXデータを送信する。


# データのやり取り(Agentで受け取るデータ)

## ActorInit

Graph(隣接行列):  
[
    0, 0, 0, 0, 0,
    1, 0, 0, 0, 0,
    1, 0, 0, 0, 0,
    0, 1, 0, 0, 0,
    0, 0, 0, 1, 0
]

NodeSize(サイズ):5



## PushData

Id(AgentのId, URLで受け取る):0

State(環境データ):
[
    [3.0, 1.0, 3.1, 4.0],
    [3.0, 1.0, 3.1, 4.0],
    [3.0, 1.0, 3.1, 4.0],
    [3.0, 1.0, 3.1, 4.0],
    [3.0, 1.0, 3.1, 4.0]
]

Action(行動データ):
[
    [1.0],
    [0.3],
    [0.3],
    [0.3],
    [0.3]
]

Reward(報酬):0.5

Done(終了フラグ):false
Z


# Writerで行う動作
## Writer(save)
　resume.json内部に学習状態を保存しておく、保存モデルは最新のものだけ置いておくことにする。保存されるモデルは

```
policy.pth
policy.onnx
q_function_1.pth
q_function_2.pth
value.pth
```

の五つ、学習状態として保存しておくデータは

```resume.json
step(update回数): 100
```

　保存の際には、ライターのメンバにpolicy.onnxの